# Deep reinforcemnet learning from Human preferences 



We explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments.This approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. We can successfully train complex novel behaviors with about an hour of human time.

Unfortunately, many tasks involve goals that are complex, poorly-defined, or hard to specify.It’s not clear how to construct a suitable reward function. We could try to design a simple reward function that approximately captures the intended behavior, but this will often result in behavior that optimizes our reward function without actually satisfying our preferences.

This difficulty underlies recent concerns about misalignment between our values and the objectives of our RL systems

If we could successfully communicate our actual objectives to our agents, it would be a significant step towards addressing these concerns. If we have demonstrations of the desired task, we can extract a reward function using inverse
reinforcement learning. More directly, we can use imitation learning to clone the demonstrated behavior.


An alternative approach is to allow a human to provide feedback on our system’s current behavior and to use this feedback to define the task

In summary, we desire a solution to sequential decision problems without a well-specified reward function that : 
* enables us to solve tasks for which we can only recognize the desired behavior, but not necessarily demonstrate it,
* allows agents to be taught by non-expert users,
* scales to large problems, and
* is economical with user feedback

Another paper fits a reward function to the human’s preferences while simultaneously training a policy to optimize the current predicted reward function. We ask the human to compare short video clips of the agent’s behavior, rather than to supply an absolute numerical score. Collecting feedback online improves the system’s performance and prevents it from exploiting weaknesses of the learned reward function. Consider continuous domains with four degrees of freedom and small discrete domains, where they can assume that the reward is linear in the expectations of hand-coded features. Physics tasks with dozens of degrees of freedom and Atari tasks with no hand-engineered features; the complexity of our environments force us to use different RL algorithms and reward models, and to cope with different algorithmic tradeoffs

Notable difference is that other papers elicit preferences over whole trajectories rather than short clips.

Other differences focus on changing our training procedure to cope with the nonlinear reward models and modern deep RL, for example using asynchronous training and ensembling.

Wilson et al. (2012) assumes that the reward function is the distance to some unknown “target” policy (which is itself a linear function of hand-coded features). They fit this reward function using Bayesian inference, and rather than performing RL they produce trajectories using the MAP estimate of the target policy. Our work could also be seen of a specific instance of the cooperative inverse reinforcement learning
framework

This framework considers a two-player game between a human and a robot interacting with an environment with the purpose of maximizing the human’s reward function. In our setting the human is only allowed to interact with this game by stating their preferences.

Key contribution is to scale human feedback up to deep reinforcement learning and to learn much more complex behaviors.

Instead of assuming that the environment produces a reward signal, we assume that there is a human overseer who can express preferences between trajectory segments. A trajectory segment is a sequence of observations and actions. Informally, the goal of the agent is to produce trajectories which are preferred by the human, while making as few queries as possible to the human. If the human’s preferences are generated by a reward function r, then our agent ought to
receive a high total reward according to r. So if we know the reward function r, we can evaluate the agent euantitatively. Ideally the agent will achieve reward nearly as high as if it had been using RL to optimize r

Sometimes we have no reward function by which we can quantitatively evaluate behavior (this is the situation where our approach would be practically useful). In these cases, all we can do is qualitatively evaluate how well the agent satisfies to the human’s preferences. We don’t assume that we can reset the system to an arbitrary state2 and so our segments generally begin from different states. This complicates the interpretation of human comparisons, but we show that our algorithm overcomes this difficulty even when the human raters have no understanding of our algorithm

At each point in time our method maintains a policy π  and a reward function estimate r^ each parametrized by deep neural networks.These networks are updated by three processes:

* The policy π interacts with the environment to produce a set of trajectories. 
* The parameters of π are updated by a traditional reinforcement learning algorithm, in order to maximize the sum of the predicted rewards.
* We select pairs of segments σ1; σ2 from the trajectories fτ 1τ ig produced in step 1, and send them to a human for comparison. The parameters of the mapping r^are optimized via supervised learning to fit the comparisons collected from the human so far.

These processes run asynchronously, with trajectories flowing from process (1) to process (2), human comparisons flowing from process (2) to process (3), and parameters for r^ flowing from process (3) to process (1).

After using r^ to compute rewards, we are left with a traditional reinforcement learning problem.

One subtlety is that the reward function r^ may be non-stationary, which leads us to prefer methods which are robust to changes in the reward function. This led us to focus on policy gradient methods, which have been applied successfully for such problems. The only hyperparameter which we adjusted was the entropy bonus for TRPO. This is because TRPO relies on the trust region to ensure adequate exploration, which can lead to inadequate exploration if the reward function is changing

The human judgments are recorded in a database D of triples σ1; σ2; µ, where σ1 and σ2 are the two segments and µ is a distribution over f1; 2g indicating which segment the user preferred. If the human selects one segment as preferable, then µ puts all of its mass on that choice. If the human marks the segments as equally preferable, then µ is uniform. Finally, if the human marks the segments as incomparable, then the comparison is not included in the database.

We can interpret a reward function estimate r^ as a preference-predictor if we view r^ as a latent factor explaining the human’s judgments and assume that the human’s probability of preferring a segment σi depends exponentially on the value of the latent reward summed over the length of the clip . We choose r^ to minimize the cross-entropy loss between these predictions and the actual human labels:

This follows the Bradley-Terry model (Bradley and Terry, 1952) for estimating score functions from pairwise preferences, and is the specialization of the Luce-Shephard choice rule (Luce, 2005; Shepard, 1957) to preferences over trajectory segments. It can be understood as equating rewards with a preference ranking scale analogous to the famous Elo ranking system developed for chess. Just as the difference in Elo points of two chess players estimates the probability of one player defeating the other in a game of chess, the difference in predicted reward of two trajectory segments estimates the probability that one is chosen over the other by the human. Our actual algorithm incorporates a number of modifications to this basic approach

We fit an ensemble of predictors, each trained on D triples sampled from D with replacement. The estimate r^ is defined by independently normalizing each of these predictors and then averaging the results

* A fraction of 1/e of the data is held out to be used as a validation set for each predictor. 
* We use L2 regularization and adjust the regularization coefficient to keep the validation loss between 1.1 and 1.5 times the training loss. 
* In some domains we also apply dropout for regularization 
* Rather than applying a softmax directly as described in Equation 1, we assume there is a10% chance that the human responds uniformly at random. Conceptually this adjustment is needed because human raters have a constant probability of making an error, which doesn’t decay to 0 as the difference in reward difference becomes extreme

We decide how to query preferences based on an approximation to the uncertainty in the reward function estimator, we sample a large number of pairs of trajectory segments of length k, use each reward predictor in our ensemble to predict which segment will be preferred from each pair, and then select those trajectories for which the predictions have the highest variance across ensemble members. Ideally, we would want to query based on the expected value of information of the query. The agent learns about the goal of the task only by asking a human which of two trajectory segments is better. Our goal is to solve the task in a reasonable amount of time using as few queries as possible. Real human feedback is typically only slightly less effective than the synthetic feedback; depending on the task human feedback ranged from being half as efficient as ground truth feedback to being equally efficient.The ultimate purpose of human interaction is to solve tasks for which no reward function is available. 

* We can pick queries uniformly at random rather than prioritizing queries for which there is disagreement
* Train only one predictor rather than an ensemble (no ensemble). In this setting, we also choose queries at random, since there is no longer an ensemble that we could use to estimate disagreement.
* Train on queries only gathered at the beginning of training, rather than gathered throughout training (no online queries
* Remove the L2 regularization and use only dropout (no regularization)

In general we discovered that asking humans to compare longer clips was significantly more helpful per clip, and significantly less helpful per frame. We found that for short clips it took human raters a while just to understand the situation, while for longer clips the evaluation time was a roughly linear function of the clip length. We tried to choose the shortest clip length for which the evaluation time was linear.

Agent-environment interactions are often radically cheaper than human interaction. We show that by learning a separate reward model using supervised learning, it is possible to reduce the interaction complexity by roughly 3 orders of magnitude. Not only does this show that we can meaningfully train deep RL agents from human preferences, but also that we are already hitting diminishing returns  on further sample-complexity improvements because the cost of compute is already comparable to the cost of non-expert feedback